

```{r message = FALSE}

library(readr)
pekin_text <- read_csv("~/project/proect_kutezh/pekin_text.csv", 
    col_types = cols(file = col_number(), review_date = col_number()))
View(pekin_text)

library(quanteda)
docs = corpus(pekin_text$review_text)

textdfm = quanteda::dfm(docs, what="word",
              tolower = TRUE, 
              remove_numbers = TRUE, remove_punct = TRUE,
              remove = c (stopwords("english"), stopwords("SMART"), "place", "china", "u", "good", "restaurant", "food", "shanghai", "service", "one", "get", "can", "us", "menu", "two", "well", "made", "great", "time", "will", "go", "just", "nice", "people", "staff", "came", "like", "try", "must", "recommend", "recommended", "day", "price", "also", "fun", "rmb", "always", "trip", "small", "trip", "atmosphere"))



dfmforTopics = convert(textdfm, to = "topicmodels")
```

### Построение модели

Построим модель с 30 темами

```{r message = FALSE}
library(topicmodels)
review40_lda <- LDA(dfmforTopics, k = 30, control = list(seed = 12345))
review40_lda
```

### Тема как смесь слов

Посмотрим на самые популярные слова в каждой теме. Для этого нам нужно сначала получить вероятности того, что слово относится к той или иной теме (per-topic-per-word probabilities), обозначаемые $\beta$ (*beta*)

```{r message = FALSE}
library(tidytext)
amazon2_topics1 <- tidy(review40_lda, matrix = "beta")
amazon2_topics
```

Посмотрим на популярные слова в каждой теме

```{r message = FALSE}
library(ggplot2)
library(dplyr)



amazon2_topics1$topic <- as.numeric(amazon2_topics1$topic)
amazon2_topics1 <-  group_by(amazon2_topics1, topic)
amazon2_topics1 <-    top_n(amazon2_topics1, 10, beta)
amazon2_topics1 <-    ungroup(amazon2_topics1)
amazon2_topics1<-   arrange(amazon2_topics1, topic, -beta)
 

amazon2_topics1 %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```

###Адекватные топики
Топики чисто по пекину
1 - веганский
2 - возможно мексиканская еда
4 - французская
5 - американские стейки
6 - чисто италия, пицца, сыр
7 - франция, десерты
9 - вино. вино. винишко
10 - местное, очень острое
11 - турецкая
13 - китайские пельмешки
15 - салаты
16 - испанская кухня
18 - японские суши (бизнес)
21 - чисто китайская, местная еда
26 - сладкая уточка и курица
27 - в питере пить, бары, выпивка


### Документы как смесь тем

Посмотрим, как темы распределяются по отзывам. Получим вероятности того, что документ относится к той или иной теме (per-document-per-topic probabilities), обозначаемые $\gamma$ (*gamma*)

```{r}
amazon2_documents <- tidy(review40_lda, matrix = "gamma")
amazon2_documents
```

text1, text2, ... -- это отзывы в том порядке, в каком они были при создании матрицы частот. Т.к. мы в первоначальном датасете с тех пор ничего не меняли, то для удобства и сопоставления добавим еще один столбец и объединим

```{r}
pekin_text$document = paste0("text", rownames(pekin_text))
amazon2_documents  = left_join(amazon2_documents, pekin_text, by = "document")
```

Например, отзыв 400

```{r}
filter(amazon2_documents, document == "text1") %>% select(title, document, topic, gamma) 
x1 <- filter(amazon2_documents,gamma>0.064)
```

Распределение отзывов по темам

```{r}
x2 <- filter(x1,number==1)
x2 <- filter(x2,gamma==max(gamma))
a1 <- data.frame(x2$number, x2$topic)
rownames()


```
ШАНХАЙ

```{r}

library(readr)
p_rest_trip <- read_csv("~/project/proect_kutezh/p_rest_trip.csv", 
    col_types = cols(file = col_number(), 
        review_date = col_number(), review_stars = col_number()))
View(p_rest_trip)
docs1 = corpus(p_rest_trip$review_text)

textdfm = quanteda::dfm(docs1, what="word",
              tolower = TRUE, 
              remove_numbers = TRUE, remove_punct = TRUE,
              remove = c (stopwords("english"), stopwords("french"), stopwords("SMART"), "place", "china", "u", "good", "restaurant", "food", "shanghai", "service", "one", "get", "can", "us", "menu", "two", "well", "made", "great", "time", "will", "go", "just", "nice", "people", "staff", "came", "like", "try", "must", "recommend", "recommended", "day", "price", "also", "fun", "rmb", "always", "trip", "small", "trip", "atmosphere", "en", "pm", "excellent", "best", "beijin"))
dfmforTopics1 = convert(textdfm, to = "topicmodels")
review40_lda1 <- LDA(dfmforTopics1, k = 30, control = list(seed = 12345))
amazon2_topics2 <- tidy(review40_lda1, matrix = "beta")
amazon2_topics2$topic <- as.numeric(amazon2_topics2$topic)
amazon2_topics2 <-  group_by(amazon2_topics2, topic)
amazon2_topics2 <-    top_n(amazon2_topics2, 10, beta)
amazon2_topics2 <-    ungroup(amazon2_topics2)
amazon2_topics2<-   arrange(amazon2_topics2, topic, -beta)
 

amazon2_topics2 %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

p_rest_trip<-transform(p_rest_trip, city="Beijing", xxx ="1")
pekin_text<-transform(pekin_text, city="Shanghai", xxx="1")
p_rest_trip <-select(p_rest_trip, -review_date)
pekin_text <-select(pekin_text, -review_date, -X1,-number,-link,-title)
p_rest_trip1 <-select(p_rest_trip, file, review_text)
pekin_text1 <-select(pekin_text, number, review_text)

aal <-rbind(pekin_text, p_rest_trip)


```


рест-топик

```{r}

pekin_text <- (pekin_text)
which.max(pekin_text, number=1, gamma)
```
