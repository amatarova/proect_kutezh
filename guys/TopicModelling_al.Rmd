

```{r message = FALSE}

library(readr)
pekin_text <- read_csv("~/project/proect_kutezh/pekin_text.csv", 
    col_types = cols(review_date = col_number()))
View(pekin_text)

library(quanteda)
docs = corpus(pekin_text$review_text)

textdfm = quanteda::dfm(docs, what="word",
              tolower = TRUE, 
              remove_numbers = TRUE, remove_punct = TRUE,
              remove = c (stopwords("english"), stopwords("SMART"), "place", "china", "u", "good", "restaurant", "food", "shanghai", "service", "one", "get", "can", "us", "menu", "two", "well", "made", "great", "time", "will", "go", "just", "nice", "people", "staff", "came", "like", "try", "must", "recommend", "recommended", "day", "price", "also", "fun", "rmb", "always", "trip", "small", "trip", "atmosphere"))



dfmforTopics = convert(textdfm, to = "topicmodels")
```

### Построение модели

Построим модель с 30 темами

```{r message = FALSE}
library(topicmodels)
review40_lda <- LDA(dfmforTopics, k = 30, control = list(seed = 12345))
review40_lda
```

### Тема как смесь слов

Посмотрим на самые популярные слова в каждой теме. Для этого нам нужно сначала получить вероятности того, что слово относится к той или иной теме (per-topic-per-word probabilities), обозначаемые $\beta$ (*beta*)

```{r message = FALSE}
library(tidytext)
amazon2_topics1 <- tidy(review40_lda, matrix = "beta")
amazon2_topics
```

Посмотрим на популярные слова в каждой теме

```{r message = FALSE}
library(ggplot2)
library(dplyr)



amazon2_topics1$topic <- as.numeric(amazon2_topics1$topic)
amazon2_topics1 <-  group_by(amazon2_topics1, topic)
amazon2_topics1 <-    top_n(amazon2_topics1, 10, beta)
amazon2_topics1 <-    ungroup(amazon2_topics1)
amazon2_topics1<-   arrange(amazon2_topics1, topic, -beta)
 

amazon2_topics1 %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```

###Адекватные топики
Топики чисто по пекину
1 - веганский
2 - возможно мексиканская еда
4 - французская
5 - американские стейки
6 - чисто италия, пицца, сыр
7 - франция, десерты
9 - вино. вино. винишко
10 - местное, очень острое
11 - турецкая
13 - китайские пельмешки
15 - салаты
16 - испанская кухня
18 - японские суши (бизнес)
21 - чисто китайская, местная еда
26 - сладкая уточка и курица
27 - в питере пить, бары, выпивка


### Документы как смесь тем

Посмотрим, как темы распределяются по отзывам. Получим вероятности того, что документ относится к той или иной теме (per-document-per-topic probabilities), обозначаемые $\gamma$ (*gamma*)

```{r}
amazon2_documents <- tidy(review40_lda, matrix = "gamma")
amazon2_documents
```

text1, text2, ... -- это отзывы в том порядке, в каком они были при создании матрицы частот. Т.к. мы в первоначальном датасете с тех пор ничего не меняли, то для удобства и сопоставления добавим еще один столбец и объединим

```{r}
pekin_text$document = paste0("text", rownames(pekin_text))
amazon2_documents  = left_join(amazon2_documents, pekin_text, by = "document")
```

Например, отзыв 400

```{r}
filter(amazon2_documents, document == "text1") %>% select(title, document, topic, gamma) 
x1 <- filter(amazon2_documents,gamma>0.064)
```

Распределение отзывов по темам

```{r}

ggplot(data = amazon2_documents, aes(x = factor(topic), y = gamma)) +
  geom_boxplot() +
  facet_wrap(~ title)
```


**Ваша очередь:** 

Скорее всего  для такого разнообразия текстов двух тем мало. Попробуйте выделить 5-7 тем. Как происходит разделение в этом случае?

```{r}
review5_lda <- LDA(dfmforTopics, k = 5, control = list(seed = 1234))

amazon5_topics <- tidy(review5_lda, matrix = "beta")

amazon5_top_terms <- amazon5_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

amazon5_top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```







