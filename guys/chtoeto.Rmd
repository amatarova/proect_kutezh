

```{r message = FALSE}



library(quanteda)
docs = corpus(a20$review_text)

textdfm = quanteda::dfm(docs, what="word",
              tolower = TRUE, 
              remove_numbers = TRUE, remove_punct = TRUE,
              remove = c (stopwords("english"), stopwords("SMART"), "place", "china", "u", "good", "restaurant", "food", "shanghai", "service", "one", "get", "can", "us", "menu", "two", "well", "made", "great", "time", "will", "go", "just", "nice", "people", "staff", "came", "like", "try", "must", "recommend", "recommended", "day", "price", "also", "fun", "rmb", "always", "trip", "small", "trip", "atmosphere"))



dfmforTopics = convert(textdfm, to = "topicmodels")
```

### Построение модели

Построим модель с 30 темами

```{r message = FALSE}
library(topicmodels)
review40_lda <- LDA(dfmforTopics, k = 30, control = list(seed = 12345))
review40_lda
```

### Тема как смесь слов

Посмотрим на самые популярные слова в каждой теме. Для этого нам нужно сначала получить вероятности того, что слово относится к той или иной теме (per-topic-per-word probabilities), обозначаемые $\beta$ (*beta*)

```{r message = FALSE}
library(tidytext)
amazon2_topics <- tidy(review40_lda, matrix = "beta")
amazon2_topics
```

Посмотрим на популярные слова в каждой теме

```{r message = FALSE}
library(ggplot2)
library(dplyr)



amazon2_topics$topic <- as.numeric(amazon2_topics$topic)
amazon2_topics <-  group_by(amazon2_topics, topic)
amazon2_topics <-    top_n(amazon2_topics, 10, beta)
amazon2_topics <-    ungroup(amazon2_topics)
amazon2_topics<-   arrange(amazon2_topics, topic, -beta)
 

amazon2_topics %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```

###Адекватные топики
Топики чисто по шанхаю
1 - пельмешки
2 - спорт-бар
3 - Хого, острая китайская
4 - турецкая
5 - бар
6 - пельмешки
7 - здоровая еда
9 - джаз бар, музон
10 - китайяская утка
11 - кофе, ланч
12 - индийская
13 - вино и стейки
14 - русские веганы (ват)
15 - французская
16 - турецкая
17 Cantonese cuisin
18 - италия
19 мексиканская
20 стейки
22 - тайская
23 - турецкая
25 - sichuan
26 - japanese
27 - испания
29 - пицца италия
30 - бар италия


```{r}
amazon2_documents <- tidy(review40_lda, matrix = "gamma")
amazon2_documents
```

text1, text2, ... -- это отзывы в том порядке, в каком они были при создании матрицы частот. Т.к. мы в первоначальном датасете с тех пор ничего не меняли, то для удобства и сопоставления добавим еще один столбец и объединим

```{r}
pekin_text$document = paste0("text", rownames(pekin_text))
amazon2_documents  = left_join(amazon2_documents, pekin_text, by = "document")
```

Например, отзыв 400

```{r}
filter(amazon2_documents, document == "text1") %>% select(title, document, topic, gamma) 
x1 <- filter(amazon2_documents,gamma>0.064)
```

Распределение отзывов по темам

```{r}
x2 <- filter(x1,number==1)
x2 <- filter(x2,gamma==max(gamma))
a1 <- data.frame(x2$number, x2$topic)
rownames()


```
ШАНХАЙ

```{r}

library(readr)
p_rest_trip <- read_csv("~/project/proect_kutezh/p_rest_trip.csv", 
    col_types = cols(file = col_number(), 
        review_date = col_number(), review_stars = col_number()))
View(p_rest_trip)
docs1 = corpus(p_rest_trip$review_text)

textdfm = quanteda::dfm(docs1, what="word",
              tolower = TRUE, 
              remove_numbers = TRUE, remove_punct = TRUE,
              remove = c (stopwords("english"), stopwords("french"), stopwords("SMART"), "place", "china", "u", "good", "restaurant", "food", "shanghai", "service", "one", "get", "can", "us", "menu", "two", "well", "made", "great", "time", "will", "go", "just", "nice", "people", "staff", "came", "like", "try", "must", "recommend", "recommended", "day", "price", "also", "fun", "rmb", "always", "trip", "small", "trip", "atmosphere", "en", "pm", "excellent", "best", "beijin"))
dfmforTopics1 = convert(textdfm, to = "topicmodels")
review40_lda1 <- LDA(dfmforTopics1, k = 30, control = list(seed = 12345))
amazon2_topics2 <- tidy(review40_lda1, matrix = "beta")
amazon2_topics2$topic <- as.numeric(amazon2_topics2$topic)
amazon2_topics2 <-  group_by(amazon2_topics2, topic)
amazon2_topics2 <-    top_n(amazon2_topics2, 10, beta)
amazon2_topics2 <-    ungroup(amazon2_topics2)
amazon2_topics2<-   arrange(amazon2_topics2, topic, -beta)
 

amazon2_topics2 %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

p_rest_trip<-transform(p_rest_trip, city="Beijing", xxx ="1")
pekin_text<-transform(pekin_text, city="Shanghai", document=1:9467)
p_rest_trip <-select(p_rest_trip, -review_date)
pekin_text <-select(pekin_text, -review_date, -X1,-number,-link,-title)
p_rest_trip1 <-select(p_rest_trip, file, review_text)
pekin_text1 <-select(pekin_text, number, review_text)




```


рест-топик

```{r}
amazon2_documents <- tidy(review40_lda, matrix = "gamma")
chapter_classifications <- amazon2_documents %>%
  group_by(document) %>%
  top_n(1) %>%
  ungroup()

write.csv(chapter_classifications, "chapter_classifications.csv", row.names = F)

chapter_classifications <- read_csv("~/project/proect_kutezh/guys/chapter_classifications.csv", 
    col_types = cols(document = col_number()))
View(chapter_classifications)

names(a20)[names(a20)=="file"] <- "document"

shanghai_topic <-   left_join(a20, chapter_classifications, by = "document")

shanghai_topic$topic[shanghai_topic$topic == "1"] <- "dumpling restaurant"
shanghai_topic$topic[shanghai_topic$topic == "2"] <- "sport bar" #новый тег
shanghai_topic$topic[shanghai_topic$topic == "3"] <- "hot pot house"
shanghai_topic$topic[shanghai_topic$topic == "4"] <- "Turkish cuisine" #новый тег
shanghai_topic$topic[shanghai_topic$topic == "5"] <- "bar"
shanghai_topic$topic[shanghai_topic$topic == "6"] <- "dumpling restaurant"
shanghai_topic$topic[shanghai_topic$topic == "7"] <- "Healthy" #новый тег
shanghai_topic$topic[shanghai_topic$topic == "9"] <- "Jazz bar"#новый тег
shanghai_topic$topic[shanghai_topic$topic == "10"] <- "Chinese cuisine"
shanghai_topic$topic[shanghai_topic$topic == "11"] <- "cafe"
shanghai_topic$topic[shanghai_topic$topic == "12"] <- "Indian cuisine"
shanghai_topic$topic[shanghai_topic$topic == "13"] <- "steakhouse"
shanghai_topic$topic[shanghai_topic$topic == "14"] <- "vegetarian restaurant"
shanghai_topic$topic[shanghai_topic$topic == "15"] <- "French cuisine"
shanghai_topic$topic[shanghai_topic$topic == "16"] <- "Turkish cuisine" #новый тег
shanghai_topic$topic[shanghai_topic$topic == "17"] <- "Cantonese cuisine"
shanghai_topic$topic[shanghai_topic$topic == "18"] <- "Italian cuisine"
shanghai_topic$topic[shanghai_topic$topic == "19"] <- "Mexican cuisine"#новый тег
shanghai_topic$topic[shanghai_topic$topic == "20"] <- "steakhouse"
shanghai_topic$topic[shanghai_topic$topic == "22"] <- "Taiwanese cuisine"
shanghai_topic$topic[shanghai_topic$topic == "23"] <- "Turkish cuisine" #новый тег
shanghai_topic$topic[shanghai_topic$topic == "25"] <- "Sichuan cuisine"
shanghai_topic$topic[shanghai_topic$topic == "26"] <- "Japanese cuisine"
shanghai_topic$topic[shanghai_topic$topic == "27"] <- "Spanish cuisine"
shanghai_topic$topic[shanghai_topic$topic == "29"] <- "Italian cuisine"
shanghai_topic$topic[shanghai_topic$topic == "30"] <- "Italian cuisine"

















a20$document <-as.numeric(a20$document)
as.numeric(chapter_classifications$document)

# Приводим все слова к нижнему регистру
pekin_text$review_text = tolower(pekin_text$review_text)


# Уберем всю пунктуацию
pekin_text$review_text = stringr::str_replace_all(pekin_text$review_text, "[[:punct:]]", "")
 # Убирем то, что осталось от кавычек и апострофа
pekin_text$review_text = stringr::str_replace_all(pekin_text$review_text, "\\&quot\\;", " ")
pekin_text$review_text = stringr::str_replace_all(pekin_text$review_text, "\\&apos\\;", " ") 

pekin_text$review_text = as.character(pekin_text$review_text)
pekin_text$file = as.character(pekin_text$file)

y = select(pekin_text,review_text, file)

func_paste <- function(x) paste(unique(x), collapse = ', ')
a20 = y %>%
group_by(file) %>%
summarise_each(funs(func_paste))
             


```

func_paste <- function(x) paste(unique(x), collapse = ', ')
a20 = pekin_text %>%
group_by(review_text) %>%
summarise_each(funs(func_paste))
write.csv(pekin_text, "shan.csv", row.names = F)

```
