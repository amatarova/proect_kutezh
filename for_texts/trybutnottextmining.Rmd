gde ti
```{r setup, include=FALSE}
install.packages("tmcn", repos="http://R-Forge.R-project.org", type="source")
install.packages("Rwordseg", repos="http://R-Forge.R-project.org")
library(tidytext)
library(ggplot2)
library(tm)
library(RColorBrewer)
library(tidyr)
library(dplyr)
library(stringr)
library(readr)
library(tmcn)
library(Rwordseg)

ch_rev = read.csv("/students/dvchuprina/proect_kutezh/ch_rev.csv")
stop_words = read.table("~/proect_kutezh/stopwords-zh.txt")
#tr_mcc_codes = tidyr::separate(tr_mcc_codes, mcc_codemcc_description, c("mcc_code", "mcc_description"), ";")
names(stop_words)[1] = "word"
stop_words$word <- as.character(stop_words$word)
# write.csv(addr, "stop_words.csv")
rev_train <- data.frame(head(ch_rev,500))

write.csv(stop_words, "stop_words.csv")
rev_train$restId = rev_train$restId %>% as.factor()
rev_train$content = rev_train$content %>% as.character()

```

```{r}
#  готовые стоп-слова
stop = as.data.frame(stop_words[-c(24,41),])
stop_w_ready = stop
write.csv(stop_w_ready, "stop_w_ready.csv", row.names = F)
names(stop_w_ready)[1] = "word"
```
из кит сайтика не работают пока, надо вставить словари тут
```{r}

words <- readLines("http://wubi.sogou.com/dict/download_txt.php?id=9182")
words <- toTrad(words)
insertWords(words)

```

500 reviews together and corpus
```{r}
rev_text <- paste(rev_train$content, collapse=" ")
rev_source <- VectorSource(rev_text)
corpus <- Corpus(rev_source, readerControl = list(reader=readPlain, language="cn"))

corpus <- tm_map(corpus[1:100], segmentCN, nature = TRUE)
corpus <- tm_map(corpus, function(sentence) {
    noun <- lapply(sentence, function(w) {
        w[names(w) == "n"]
    })
    unlist(noun)
})
corpus <- Corpus(VectorSource(corpus))
```
udalyaem stopwords
```{r}
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- corpus %>%
  filter(!word1 %in% stop_w_ready$word) %>%
  filter(!word2 %in% stop_w_ready$word)
stopwords <- c(stopwordsCN(), "編輯", "時間", "標題", "發信", "實業", "作者")
head(stopwords, 20)
corpus <- tm_map(corpus, removeWords, stopwords ('chinese'))
```



Delaem bigrams don't know why
```{r}
bigrams = rev_train %>%
    unnest_tokens(bigram, content, token = "ngrams", n = 2)

bigrams_separated <- bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

bigrams_counts <- bigrams_filtered %>% count(word1,word2,sort = TRUE)
  
# View(bigrams_counts)
  
bigrams_united <- bigrams_filtered %>%
unite(bigram, word1, word2, sep = " ")

# View(bigrams_united)

bigram_tf_idf <- bigrams_united %>%
  count(restId, bigram) %>%
  bind_tf_idf(bigram, restId, n_col=n) %>%
  arrange(desc(tf_idf))
```
оно работает, но не ясно что это

дальше идет какая-то херь
```{r}
plot_bigram <- bigram_tf_idf %>% 
  group_by(restId) %>% 
  top_n(15) %>%
  mutate(bigram = reorder(bigram, tf_idf))

ggplot(plot_bigram, aes(bigram, tf_idf, fill = restId)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~restId, scales = "free") +
  coord_flip()
```
correlation не вышла ошибка 
```{r}
library(widyr)

reviews <- rev_train %>%
  unnest_tokens(word, content) %>%
  anti_join(stop_words)

word_cors <- content() %>%
  group_by(word) %>%
  filter(n() > 30) %>%
  pairwise_cor(word, id, sort = TRUE)

rm(ch_rev)
```

```{r}
stop = stop_words[-c(1,2),]
stop = as.data.frame(stop)
```


